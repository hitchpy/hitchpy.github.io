<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Yu's Website</title>
		<description>Talk data to me.</description>
		<link>http://hitchpy.github.io</link>
		<atom:link href="http://hitchpy.github.io/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Creative Writing and Research Paper Writing</title>
				<description>&lt;p&gt;Many people will agree with me that writing scientific manuscript is easier than creative writing. For papers, each section is clearly defined. You layout the background, list your contributions first, then &lt;em&gt;just&lt;/em&gt; describe what you have done and how you did it. Related works will take some time, but you know what they have done, how their work differ from yours, so no big issues here.&lt;/p&gt;

&lt;p&gt;Is it good that you can write your paper in a very engaging way? Of course! Here I would highly recommend every aspiring scientists to watch this fantastic short presentation by Prof. Simon Peyton Jones about writing &lt;strong&gt;great&lt;/strong&gt; research paper &lt;a href=&quot;https://www.youtube.com/watch?v=g3dkRsTqdDA&quot;&gt;Link&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;Turns out, much like novels, a research paper is telling a story too. You have already done a lot of good work, made some great discoveries, now you want to attract your readers with your convincing arguments and engaging descriptions. And that’s where creative writing comes in. It trains you to emphasize the main story lines, create the necessary details while gloss over the unimportant parts. &lt;/p&gt;

&lt;p&gt;That’s exactly what I have been doing lately, training myself to be a better writing by reading various novels. &lt;/p&gt;

&lt;p&gt;Some people might say: “No, Yu, this is just your way to justify reading that much novels!”&lt;/p&gt;

&lt;p&gt;But I will reason with them: “Of course not, this blog post is a proof that my writing is definitely getting better.”&lt;/p&gt;
</description>
				<pubDate>Mon, 18 Apr 2016 00:00:00 -0400</pubDate>
				<link>http://hitchpy.github.io/2016/04/Creative-writing-and-research-paper-writing</link>
				<guid isPermaLink="true">http://hitchpy.github.io/2016/04/Creative-writing-and-research-paper-writing</guid>
			</item>
		
			<item>
				<title>New path</title>
				<description>&lt;p&gt;Year 2015 is coming to an end. Now would be a good time to summarize what I have been doing recently, and talk a little bit more about what happens next(Spoiler, I am moving to Tennessee!).&lt;/p&gt;

&lt;p&gt;Around August, I was in a state full of uncertainties. Projects on Kaggle are interesting but they are side projects - something to explore over the weekends when you have a &lt;strong&gt;job&lt;/strong&gt;. There were a few things going on then: I decided to stay in the bioenergy project only to the end of this year; find data analyst/scientist jobs and go into industry; a PhD opportunity in high performance computing in University of Tennessee, Knoxville. &lt;/p&gt;

&lt;p&gt;Finding job is a painful and time-consuming process, but if you set your mind to it, there are positions available. But even with a master degree, the job most likely will be basic data processing etc. On the other hand, the PhD program will be in a prestige research group, and there are large projects with important engineering and science questions to work on. I talked with several my friends and most of them have doubts about time investment in PhD, and location of the school(too “countryside”). Those are all valid concerns, but after some serious thoughts, I believe this is what I want to do and a choice I won’t regret. &lt;/p&gt;

&lt;p&gt;With those cleared out of my mind, I focused on tying up everything here in the project and that is what I have been doing for the past few months. I need to write manuscript and publish the work we have done. Manuscript writing is something you need to practice to get good at, and I am definitely feeling the initial pain. Every procedure, analysis must be checked, everything need to be documented and well organized, and most importantly, I need to be patient. It is a long process: articulate the objective, perform necessary experiments to test hypothesis, interpret the result, write out the draft, find some errors, REPEAT… Finally we have our first draft ready to submit(yay!), let’s wait and see what will happen next.&lt;/p&gt;

&lt;p&gt;With enough said about the past, let’s set some goals for 2016. My mentor has high hopes and expectations for me, and I do not plan to let him down. The transition from mostly writing scripts to analyze data to build efficient infrastructure for computation won’t be easy, fortunately, there will be many people in the new place to help(yes please!). My goal will be stay focused, and learn the things I need to accomplish the work.&lt;/p&gt;

&lt;p&gt;New path lies ahead, and I ready for it.&lt;/p&gt;
</description>
				<pubDate>Fri, 18 Dec 2015 00:00:00 -0500</pubDate>
				<link>http://hitchpy.github.io/2015/12/New-path</link>
				<guid isPermaLink="true">http://hitchpy.github.io/2015/12/New-path</guid>
			</item>
		
			<item>
				<title>Time to document some thoughts</title>
				<description>&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Recently I have spend some time working on two Kaggle competitions. One is a image classification problem &lt;a href=&quot;https://www.kaggle.com/c/diabetic-retinopathy-detection&quot;&gt;Diabetic Retinopathy&lt;/a&gt;, the other one is context Ad click through rate(CTR) prediction &lt;a href=&quot;https://www.kaggle.com/c/avito-context-ad-clicks&quot;&gt;Avito Context Ad Clicks&lt;/a&gt;. You can find my solutions &lt;a href=&quot;https://github.com/hitchpy/Kaggle-Diabetic-Retinopathy-Detection&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://github.com/hitchpy/Kaggle-Avito-Context-Ad&quot;&gt;here&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Both are very interesting topics, but I didn’t have enough time to explore the problem more deeply and the models are definitely suboptimal. Still, I have learned a few things from them. &lt;/p&gt;

&lt;h3 id=&quot;image-classification-and-convolution-neural-networkcnn&quot;&gt;Image classification and convolution neural network(CNN)&lt;/h3&gt;
&lt;p&gt;Early this year, Stanford offered a class regarding CNN for visual recognition and they kindly shared all the wonderful notes and homeworks &lt;a href=&quot;http://cs231n.stanford.edu/index.html&quot;&gt;online&lt;/a&gt;. I have followed the class and worked on the problems. And this Diabetic problem is the perfect opportunity to apply those knowledges and tried out Theano(in this case, the wonderful &lt;a href=&quot;http://keras.io/&quot;&gt;Keras&lt;/a&gt; wrapper package). Since I have zero prior image processing experience, I mostly just want to get a feel of CNN. I read through a few of the top competitors’ solution and code, those are some serious engineering efforts! Just some low hanging fruits that might help improving my current model:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;further image preprocessing will be very helpful. including color adjustments, and normalization with population mean and standard deviation. And it seems like class balancing is not that important, some other kind of augmentation schemes will be useful(flipping etc.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Customizing the loss function will help. Initially I use cross-entropy for multi-class classification loss. But the model just wasn’t learning anything(I was using the original pictures with huge black frames!). As pointed out by others, the classes are ordered, so a metric that can reflect this would be more appropriate. And a soft version of Kappa is also possibly better. I used MSE coupled with ranked score and cutoff, which seems to be a reasonable method too.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This is indeed a huge project and I really admire those people that dedicated few months time to tackle this problem. I tried one VGG structure in the last two weeks and that was all I ever tried! Experimenting with nonlinearity and stride size, model depth certainly will yield meaning results. But I didn’t do anything! Not even did any model validation! &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;online-learning-and-ctr-estimation&quot;&gt;Online learning and CTR estimation&lt;/h3&gt;

&lt;p&gt;For many fellow Kaggler, this type of problem is nothing new. There were a few similar competitions before this already, and the methods used for this kind of problem is very standard too(Logistic regression, FFM etc). But there are always new twist to the problem. For this competition:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The data is fairly large, so training and model iteration will be more difficult.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There are different kind of Ads, and “Ad” is like the content of the webpage, not like what is usually showed on a sidebar. The data is very rich and you can generate a lot of useful features from it. With visits history, you can do personalization if you are really serious. But I guess it must be very time consuming and will not fit into the online learning framework, so even the top rankers are not doing it. Although with the search query and titles and parameters of the product, some people did NLP analysis and yield some useful feature.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Although FTRL is mostly logistic regression under the hook, they never teach you to use features sets of millions(with no regularization) in statistic class. Using AdID and IPID as features seems too raw, but it did yield better result.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A “so obvious” after they told you feature is to combine all the results from the same research(their position, type, etc) as a tuple, and treated it as categorical features. It makes intuitive sense, since whether a user will click on a particular ad will certainly depends on the whole search result, not just one single ad.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One last question I have is really how “good” first place’s model is, what does 0.0402 logloss means in this instant, and how big a difference it actually is between 0.045 and 0.040. For a sample of 7,800,000 test points, 0.005 means a total of 39,000 total difference, and that is no a small number. &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;There are two ways you can approach Kaggle competitions. You can, like me, try something new, a type of problem you haven’t work on, and you test some basic ideas. Or you can focus on one or two problems, and dedicate your energy to cracking that problem. I have tried the first style quite a bit, I might need to try the second one now. Just keep at it!&lt;/p&gt;
</description>
				<pubDate>Tue, 28 Jul 2015 00:00:00 -0400</pubDate>
				<link>http://hitchpy.github.io/2015/07/Time-to-document-some-thoughts</link>
				<guid isPermaLink="true">http://hitchpy.github.io/2015/07/Time-to-document-some-thoughts</guid>
			</item>
		
			<item>
				<title>Some thoughts on my Kaggle experiences</title>
				<description>&lt;p&gt;It really has been a while since my last blog post! Anyway, since I participated in the &lt;a href=&quot;https://www.kaggle.com/c/inria-bci-challenge&quot;&gt;BCI Kaggle competition&lt;/a&gt; with several my classmates, I entered a few more(&lt;a href=&quot;https://www.kaggle.com/c/how-much-did-it-rain&quot;&gt;Rain&lt;/a&gt;, &lt;a href=&quot;https://www.kaggle.com/c/facebook-recruiting-iv-human-or-bot&quot;&gt;Robot Detaction&lt;/a&gt;, &lt;a href=&quot;https://www.kaggle.com/c/predict-west-nile-virus&quot;&gt;Virus Prediction&lt;/a&gt; and &lt;a href=&quot;https://www.kaggle.com/c/crowdflower-search-relevance&quot;&gt;CrowdFlower Search results relevance&lt;/a&gt;). Trust me, it is not easy at the beginning. Not knowing what to do, and the vast amount of time needed for each competition probably will put a lot of people off. Here, I will share my own experiences so far from participating in all these competitions. It will be about the general thought process, how to iterate over different ideas and what I have learned from them. I am by no means a top competitor, but I want to share it anyway. And maybe some years from now, it will be funny to look back!&lt;/p&gt;

&lt;p&gt;First of all, people would say that these are competitions, how realistic or similar to real world modeling? It it true that there certainly will be some differences, but many industry products start out with prototypes and this might be similar to that. And second, some competition organizers genuinely have end product in mind and intent to make the competition more “realistic”. Of course this itself is a huge topic and I would go into detail here, but I strongly suggest &lt;a href=&quot;http://www.thetalkingmachines.com/blog/2015/6/18/working-with-data-and-machine-learning-in-advertizing&quot;&gt;this podcast episode&lt;/a&gt; and &lt;a href=&quot;https://sites.google.com/site/claudiaperlich/home&quot;&gt;The speaker’s website&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;So you are looking at the description of one competition, it seems quite interesting, a classification or regression problem. You decided to download the data and have a look, and see what other people are discussing in the forum. You might have some ideas about possible ways to tackle this problem, or there are shared starter code to get you started. The value of experiences is when you see a problem, you can probably “know” what methods might work, what kinds of features might be important for prediction. This is the most excruciating or most exhilarating part. The frustration of after exploring the data 1000 ways, trying all the methods you ever know, and things still doesn’t work. Or the joy of finding the important features and see a huge bump in your score. They are all valuable lessons. The wonderful people in Kaggle usually will share their solutions after the competition is over. And you will know why you didn’t do well and improve next time in a similar situation. It is all good!&lt;/p&gt;

&lt;p&gt;In terms of iteration of ideas, that’s where good engineering practices and master of tools come in. Knowing your tools(R packages or scikit-learn functions) well means when you have an idea, you can quickly put it to test. And good structuring or your data processing code, cross validation etc means you can swipe in and out of modules/methods quickly. And everything will be smooth..But again, this will require a lot of practice and a good habit. Usually there are a bunch of engineers in each competition, and you can learn a lot from them.&lt;/p&gt;

&lt;p&gt;Finally what I have learned from the competitions I participated? Getting Generalized linear model(GLM) to actions, working on text data, and doing various exploratory to get better features! And as classified by someone else, there are structured dominated problems and noise dominated problems(BCI with EEG data definitely is in this category). Knowing what to do(finding tiny signals or finding complicated structure) in each cases(avoid overfitting noise or use more complicated algorithm to capture the underlying functions) will be vital. &lt;/p&gt;

&lt;p&gt;At the moment I am still not very familiar with the strength and weakness of some common algorithms, don’t know how to handle many types of data, don’t know efficient way to tackle the problem if data gets too big, don’t have good engineering practices. &lt;/p&gt;

&lt;p&gt;So, back to the modeling!&lt;/p&gt;

</description>
				<pubDate>Sun, 05 Jul 2015 00:00:00 -0400</pubDate>
				<link>http://hitchpy.github.io/2015/07/Some-thoughts-on-my-Kaggle-experiences</link>
				<guid isPermaLink="true">http://hitchpy.github.io/2015/07/Some-thoughts-on-my-Kaggle-experiences</guid>
			</item>
		
			<item>
				<title>Experiences with data scientist interviews</title>
				<description>&lt;p&gt;I have been looking for jobs recently because I am about to graduate soon. 
At the beginning of February, I have two interviews for the position of (junior) data scientist. Those are my first ever interviews. Although I didn’t get offer from either of them, it was a great experience. I would like to share some of the experiences here, and some thoughts about the process. Since there were non-disclosure agreements, I will not give out detail but only the general ideas. Hopefully someone might find this useful for their own future preparation.&lt;/p&gt;

&lt;p&gt;It all began in the Engineering and Science career fair here at UC Davis around mid-Jan(quite awhile back already!). There are many tech companies but not so many have opening for data scientist position. I handed out about a dozen resumes and talked to the representatives about what I have done and why I am competent for the job. Naturally, they are all very polite and just told me to go back and wait for notice. Workday.inc provided a short interview opportunities that afternoon, and finally I was invited to have an onsite interview. The second interview was from Engage3, a startup here in Davis, focus on retailing. They called me a few days after the career fair. &lt;/p&gt;

&lt;h3 id=&quot;general-process&quot;&gt;General process&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Engage3
    &lt;ul&gt;
      &lt;li&gt;Called me, scheduled an hour informal interview with one of the senior member of the data scientist team.&lt;/li&gt;
      &lt;li&gt;Talked with him in a coffee house, about what they are doing, what visions they have about their product, and what kinds of skills they would expect from their candidates. I had quite a nice conversation. Two days later they invited me for another two hour in-depth interview.&lt;/li&gt;
      &lt;li&gt;The two hour interview includes two components, one is technical with two of the team members, the other one is behavioral. More details later.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Workday
    &lt;ul&gt;
      &lt;li&gt;Mature company focus on enterprise software, especially in human management and financial management etc. Beautiful office in downtown SF. They are rapidly expanding their data science team, and they expect it will play an important role in their company’s decisions.&lt;/li&gt;
      &lt;li&gt;During that short half hour on-campus interview, one of the senior member in the team interviewed me. It covered some basic statistical concepts, at the undergraduate level, and one SQL question, one data manipulation question. All quite standard. I didn’t ace it but I think I did ok. HR sent out an email for onsite a few days later.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;components-of-the-main-interview&quot;&gt;Components of the main interview&lt;/h3&gt;

&lt;p&gt;The term data scientist is widely used only in recently years and the role is not very well defined. Every companies have their own expectations, some more research oriented, some more software engineering related, some more like consulting etc.&lt;/p&gt;

&lt;h4 id=&quot;engage3&quot;&gt;Engage3&lt;/h4&gt;
&lt;p&gt;They were very straightforward and had clear idea about what their candidate should be able to do. They expect you know your conditional probability well(do it like a bayesian!), they also expect that you have strong hands on skill. I was given a piece of Python script to debug, since now I mostly use R for day to day analysis, it took me a while to spot the bug. No the usual kind of algorithm questions for software engineer position and that is nice. Fundamentally I think it is important that data scientist know those concepts but in it’s core, it is about analytics, the ability to explore data and yield insight from them. &lt;/p&gt;

&lt;p&gt;On the behavioral site, they focus more on your personality, whether you can endure hardship(you can imagine the importance of the trait for a startup company). In hindsight, I should be more well prepared for this interview. Review the stat/probability concepts, and brush up my Python skill since most people outside stat would be more comfortable with Python/Java. They told me they will make the final decision next week and I got the rejection right on time.&lt;/p&gt;

&lt;h4 id=&quot;workday&quot;&gt;Workday&lt;/h4&gt;
&lt;p&gt;Six round of interviews, the people there are all very talented. PhD from Physics/Stat/machine learning, experienced software engineers etc. First interview was about basic stat concepts again! &lt;strong&gt;Condition distribution&lt;/strong&gt;, can’t stress that point more. Two of the them were coding style interviews. Need to write out the solution in a whiteboard, and yes, they would prefer you do it with Python/Java. The questions weren’t hard and I wrote out the solutions with Python with some hints. One of the interview was focused on machine learning algorithms, how they works, how to train them, their pros and cons. In conclusion, to do well in a data scientist interview, you need to know your ML toolbox well! And finally the head of the team talked with me for a moment, about what they are doing and their visions. &lt;/p&gt;

&lt;p&gt;After that I also briefly interviewed with Cablevision and some other companies as well but still don’t have any offer yet. More hands on experiences are needed! &lt;/p&gt;

&lt;p&gt;Next post I will talk about the Kaggle competition [BCI Challenge]{http://www.kaggle.com/c/inria-bci-challenge}. Which me and several my classmates joined and finally got 79 out of 270 teams.&lt;/p&gt;

</description>
				<pubDate>Wed, 04 Mar 2015 00:00:00 -0500</pubDate>
				<link>http://hitchpy.github.io/2015/03/Experiences-with-data-scientist-interviews</link>
				<guid isPermaLink="true">http://hitchpy.github.io/2015/03/Experiences-with-data-scientist-interviews</guid>
			</item>
		
			<item>
				<title>Topic Modeling with Latent Dirichlet Allocation(LDA)</title>
				<description>&lt;p&gt;Once upon a time when I was an undergrad, whenever I heard people talk about MCMC or EM algorithms, I would be like: WOW, Those sounds really cool!  Now that I have Bayesian inference and MCMC under the belt, it is amazing how many things I can understand and do.&lt;/p&gt;

&lt;p&gt;This is a presentation for my MCMC course, which is more on the theory side. But we can do anything related to MCMC, theory, application etc. Naturally I went to Quora to find some inspiration. Then I came across this amazing field of Bayesian non-paramentric, which includes Dirichlet process, Gaussian process, LDA etc. Those basically are the latest and greatest hits right now and are widely using in machine learning field. So get down to business, find papers, lecture videos and start learning!&lt;/p&gt;

&lt;p&gt;LDA is a fairly simple model, it has some limitations like you need to specify the topics number beforehand. But many improvements have been proposed since David Biel etc originally proposed it in 2003. Nevertheless, still a brilliant algorithm. &lt;/p&gt;

&lt;p&gt;I would like to comb through the model specification and the modeling fitting sometime for later reference. But for now I will just post the presentation that I have. Professor Choi explained collaped Gibbs a little bit more for us, great experence! &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://s3-us-west-1.amazonaws.com/yu.public/YuPei_stat280.pdf&quot;&gt;My presentation slides&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Tue, 02 Dec 2014 00:00:00 -0500</pubDate>
				<link>http://hitchpy.github.io/2014/12/Latent-Dirichlet-Allocation-Tryout</link>
				<guid isPermaLink="true">http://hitchpy.github.io/2014/12/Latent-Dirichlet-Allocation-Tryout</guid>
			</item>
		
			<item>
				<title>test of converting Rmarkdown with plots</title>
				<description>&lt;p&gt;This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see &lt;a href=&quot;http://rmarkdown.rstudio.com&quot;&gt;http://rmarkdown.rstudio.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When you click the &lt;strong&gt;Knit&lt;/strong&gt; button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;r
summary(cars)
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
##      speed           dist    
##  Min.   : 4.0   Min.   :  2  
##  1st Qu.:12.0   1st Qu.: 26  
##  Median :15.0   Median : 36  
##  Mean   :15.4   Mean   : 43  
##  3rd Qu.:19.0   3rd Qu.: 56  
##  Max.   :25.0   Max.   :120
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;You can also embed plots, for example:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/figure/unnamed-chunk-2.png&quot; alt=&quot;plot of chunk unnamed-chunk-2&quot; /&gt; &lt;/p&gt;

&lt;p&gt;Note that the &lt;code&gt;echo = FALSE&lt;/code&gt; parameter was added to the code chunk to prevent printing of the R code that generated the plot.&lt;/p&gt;
</description>
				<pubDate>Sat, 01 Nov 2014 00:00:00 -0400</pubDate>
				<link>http://hitchpy.github.io/2014/11/Test-rmarkdown</link>
				<guid isPermaLink="true">http://hitchpy.github.io/2014/11/Test-rmarkdown</guid>
			</item>
		
			<item>
				<title>First Post!</title>
				<description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Let’s face it, we all need to communicate our ideas with others. I started this page hoping that some of my thoughts/codes can reach out to more people and I can get more feedback from others.&lt;/p&gt;

&lt;p&gt;To be a data scientist, you need to know a bunch of stuffs. And it takes time to be good at something. So the focus of the things that I do will mostly be:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Data cleaning&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Application of Statistical/ Machine learning algorithms &lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Details of algorithms/data structure implementations&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Some visualizations/result communications&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since I am trained as a statistician, &lt;strong&gt;R&lt;/strong&gt; will be my primary language, but I speak &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;C++&lt;/strong&gt; as well.&lt;/p&gt;

&lt;p&gt;Enjoy!&lt;/p&gt;
</description>
				<pubDate>Wed, 17 Sep 2014 00:00:00 -0400</pubDate>
				<link>http://hitchpy.github.io/2014/09/First-post!</link>
				<guid isPermaLink="true">http://hitchpy.github.io/2014/09/First-post!</guid>
			</item>
		
	</channel>
</rss>
