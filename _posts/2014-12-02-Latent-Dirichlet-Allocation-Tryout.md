---
layout: post
title: "Topic Modeling with Latent Dirichlet Allocation(LDA)"
date: 2014-12-02
comments: true
---


Once upon a time when I was an undergrad, whenever I heard people talk about MCMC or EM algorithms, I would be like: WOW, Those sounds really cool!  Now that I have Bayesian inference and MCMC under the belt, it is amazing how many things I can understand and do.

This is a presentation for my MCMC course, which is more on the theory side. But we can do anything related to MCMC, theory, application etc. Naturally I went to Quora to find some inspiration. Then I came across this amazing field of Bayesian non-paramentric, which includes Dirichlet process, Gaussian process, LDA etc. Those basically are the latest and greatest hits right now and are widely using in machine learning field. So get down to business, find papers, lecture videos and start learning!

LDA is a fairly simple model, it has some limitations like you need to specify the topics number beforehand. But many improvements have been proposed since David Biel etc originally proposed it in 2003. Nevertheless, still a brilliant algorithm. 

I would like to comb through the model specification and the modeling fitting sometime for later reference. But for now I will just post the presentation that I have. Professor Choi explained collaped Gibbs a little bit more for us, great experence! 

[My presentation slides](https://s3-us-west-1.amazonaws.com/yu.public/YuPei_stat280.pdf)